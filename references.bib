@Article{ioannidis2007ExploratoryTestExcess,
  title = {An Exploratory Test for an Excess of Significant Findings},
  volume = {4},
  issn = {1740-7745, 1740-7753},
  doi = {10.1177/1740774507079441},
  abstract = {Background The published clinical research literature may be distorted by the pursuit of statistically significant results.

Purpose We aimed to develop a test to explore biases stemming from the pursuit of nominal statistical significance.

Methods The exploratory test evaluates whether there is a relative excess of formally significant findings in the published literature due to any reason (e.g., publication bias, selective analyses and outcome reporting, or fabricated data). The number of expected studies with statistically significant results is estimated and compared against the number of observed significant studies. The main application uses \textvisiblespace{} Ï­ 0.05, but a range of \textvisiblespace{} thresholds is also examined. Different values or prior distributions of the effect size are assumed. Given the typically low power (few studies per research question), the test may be best applied across domains of many meta-analyses that share common characteristics (interventions, outcomes, study populations, research environment).

Results We evaluated illustratively eight meta-analyses of clinical trials with Ï¾50 studies each and 10 meta-analyses of clinical efficacy for neuroleptic agents in schizophrenia; the 10 meta-analyses were also examined as a composite domain. Different results were obtained against commonly used tests of publication bias. We demonstrated a clear or possible excess of significant studies in 6 of 8 large metaanalyses and in the wide domain of neuroleptic treatments. Limitations The proposed test is exploratory, may depend on prior assumptions, and should be applied cautiously.

Conclusions An excess of significant findings may be documented in some clinical research fields. Clinical Trials 2007; 4: 245\textendash{}253; http://ctj.sagepub.com},
  language = {en},
  number = {3},
  journal = {Clinical Trials: Journal of the Society for Clinical Trials},
  author = {John PA Ioannidis and Thomas A Trikalinos},
  month = {jun},
  year = {2007},
  pages = {245-253},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\RL3EEX84\\Ioannidis & Trikalinos (2007).pdf},
}
@Article{vanassen2015MetaanalysisUsingEffect,
  title = {Meta-Analysis Using Effect Size Distributions of Only Statistically Significant Studies.},
  volume = {20},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000025},
  language = {en},
  number = {3},
  journal = {Psychological Methods},
  author = {Marcel A. L. M. {van Assen} and Robbie C. M. {van Aert} and Jelte M. Wicherts},
  year = {2015},
  pages = {293-309},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\B3FF7NCC\\van Assen et al. (2015).pdf},
}
@Article{johnson2007CommentsExploratoryTest,
  title = {Comments on `{{An}} Exploratory Test for an Excess of Significant Findings' by {{JPA}} Loannidis and {{TA Trikalinos}}},
  volume = {4},
  issn = {1740-7745, 1740-7753},
  doi = {10.1177/1740774507079437},
  language = {en},
  number = {3},
  journal = {Clinical Trials: Journal of the Society for Clinical Trials},
  author = {Valen Johnson and Ying Yuan},
  month = {jun},
  year = {2007},
  pages = {254-255},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\UQTUAVLT\\Johnson & Yuan (2007).pdf},
}
@Article{morey2013ConsistencyTestDoes,
  title = {The Consistency Test Does Not\textendash{}and Cannot\textendash{}Deliver What Is Advertised: {{A}} Comment on {{Francis}} (2013)},
  volume = {57},
  issn = {00222496},
  shorttitle = {The Consistency Test Does Not\textendash{}and Cannot\textendash{}Deliver What Is Advertised},
  doi = {10.1016/j.jmp.2013.03.004},
  abstract = {The statistical consistency test of Ioannidis and Trikalinos (2007) has been used recently by Francis (2012a,c,d,e,2013,in press), to argue that specific sets of experiments show evidence of publication bias. I argue that the test is unnecessary because publication bias exists almost everywhere as property of the research process, not individual studies. Furthermore, for several reasons, the test does not support the claims made on its behalf. Instead of focusing on testing sets of experiments for publication bias, we should focus on changes to scientific culture to reduce the bias.},
  language = {en},
  number = {5},
  journal = {Journal of Mathematical Psychology},
  author = {Richard D. Morey},
  month = {oct},
  year = {2013},
  pages = {180-183},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\84NIY9V5\\Morey (2013).pdf},
}
@Article{ioannidis2013ClarificationsApplicationInterpretation,
  title = {Clarifications on the Application and Interpretation of the Test for Excess Significance and Its Extensions},
  volume = {57},
  issn = {00222496},
  doi = {10.1016/j.jmp.2013.03.002},
  abstract = {This commentary discusses challenges in the application of the test for excess significance (Ioannidis \& Trikalinos, 2007) including the definition of the body of evidence, the plausible effect size for power calculations and the threshold of statistical significance. Interpretation should be cautious, given that it is not possible to separate different mechanisms of bias (classic publication bias, selective analysis, and fabrication) that lead to an excess of significance and in some fields significance-related biases may follow a complex pattern (e.g. Proteus phenomenon and occasional preference for ``negative'' results). Likelihood ratio estimates can be used to generate the post-test probability of bias, and correcting effect estimates for bias is possible in theory, but may not necessarily be reliable.},
  language = {en},
  number = {5},
  journal = {Journal of Mathematical Psychology},
  author = {John P.A. Ioannidis},
  month = {oct},
  year = {2013},
  pages = {184-187},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\6Z9MY7YA\\Ioannidis (2013).pdf},
}
@Article{iyengar1988SelectionModelsFile,
  title = {Selection {{Models}} and the {{File Drawer Problem}}},
  volume = {3},
  issn = {0883-4237},
  doi = {10.1214/ss/1177013012},
  abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
  language = {en},
  number = {1},
  journal = {Statistical Science},
  author = {Satish Iyengar and Joel B. Greenhouse},
  month = {feb},
  year = {1988},
  pages = {109-117},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\TVPQGW3Q\\Iyengar & Greenhouse (1988).pdf},
}
@Article{dear1992ApproachAssessingPublication,
  title = {An {{Approach}} for {{Assessing Publication Bias Prior}} to {{Performing}} a {{Meta}}-{{Analysis}}},
  volume = {7},
  abstract = {A semi-parametrmicethodis developedforassessingpublicationbias priorto performinag meta-analysisS.ummaryestimatesfor theindividualstudiesin themeta-analysiasre assumedto have known distributionaflormS. electivepublicationis modeledusinga nonparametricweightfunctiond,efinedon thetwo-sidedp-valuescale.The shapeof theestimatedweightfunctionprovidesvisual evidenceofthepresence of bias, if it exists, and observedtrendsmay be tested using rank orderstatisticsor likelihoodratiotests. The methodis intendedas an exploratorytechniquepriorto embarkingon a standardmeta-analysis.},
  language = {en},
  number = {2},
  journal = {Statistical Science},
  author = {Keith B. G. Dear and Colin B. Begg},
  year = {1992},
  pages = {237-245},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\LSXYA7QN\\Dear, Begg - 1992 - An approach for assessing publication bias prior to performing a meta-analysis.pdf},
}
@Article{hedges1992ModelingPublicationSelection,
  title = {Modeling {{Publication Selection Effects}} in {{Meta}}-{{Analysis}}},
  volume = {7},
  issn = {0883-4237},
  doi = {10.1214/ss/1177011364},
  abstract = {Publication selection effects arise in meta-analysis when the effect magnitude estimates are observed in (available from) only a subset of the studies that were actually conducted and the probability that an estimate is observed is related to the size of that estimate. Such selection effects can lead to substantial bias in estimates of effect magnitude. Research on the selection process suggests that much of the selection occurs because researchers, reviewers and editors view the results of studies as more conclusive when they are more highly statistically significant. This suggests a model of the selection process that depends on effect magnitude via the p-value or significance level. A model of the selection process involving a step function relating the p-value to the probability of selection is introduced in the context of a random effects model for meta-analysis. The model permits estimation of a weight function representing selection along the mean and variance of effects. Some ideas for graphical procedures and a test for publication selection are also introduced. The method is then applied to a meta-analysis of test validity studies.},
  language = {en},
  number = {2},
  journal = {Statistical Science},
  author = {Larry V. Hedges},
  month = {may},
  year = {1992},
  pages = {246-255},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\M6IX4GJX\\Hedges (1992).pdf},
}
@Article{vevea1995GeneralLinearModel,
  title = {A General Linear Model for Estimating Effect Size in the Presence of Publication Bias},
  volume = {60},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/BF02294384},
  language = {en},
  number = {3},
  journal = {Psychometrika},
  author = {Jack L. Vevea and Larry V. Hedges},
  month = {sep},
  year = {1995},
  pages = {419-435},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\LNNUDS9B\\Vevea, Hedges - 1995 - A general linear model for estimating effect size in the presence of publication bias.pdf},
}
@Article{mcshane2016AdjustingPublicationBias,
  title = {Adjusting for {{Publication Bias}} in {{Meta}}-{{Analysis}}: {{An Evaluation}} of {{Selection Methods}} and {{Some Cautionary Notes}}},
  volume = {11},
  issn = {1745-6916, 1745-6924},
  shorttitle = {Adjusting for {{Publication Bias}} in {{Meta}}-{{Analysis}}},
  doi = {10.1177/1745691616662243},
  language = {en},
  number = {5},
  journal = {Perspectives on Psychological Science},
  author = {Blakeley B. McShane and Ulf B{\"o}ckenholt and Karsten T. Hansen},
  month = {sep},
  year = {2016},
  pages = {730-749},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\VZV5ZIFR\\McShane, Bockenholt, & Hansen (2016).pdf},
}
@Article{carter2018CorrectingBiasPsychology,
  title = {Correcting for Bias in Psychology: {{A}} Comparison of Meta-Analytic Methods},
  shorttitle = {Correcting for Bias in Psychology},
  doi = {10.31234/osf.io/9h3nu},
  abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, much of this work has not been tailored specifically to psychology, so it is not clear which methods work best for data typically seen in our field. Here, we present a comprehensive simulation study to examine how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We created such scenarios by simulating several levels of questionable research practices, publication bias, heterogeneity, and using study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses\textemdash{}that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change based on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts on improving the primary literature and conducting large-scale, pre-registered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.},
  language = {en},
  author = {Evan C Carter and Felix D. Sch{\"o}nbrodt and Will M Gervais and Joseph Hilgard},
  year = {2018},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\EM3GNCFG\\Carter et al. (2018) Correcting-bias-in-psychology_preprint.pdf},
}
@Article{rao1948LargeSampleTests,
  title = {Large Sample Tests of Statistical Hypotheses Concerning Several Parameters with Applications to Problems of Estimation},
  volume = {44},
  issn = {0305-0041, 1469-8064},
  doi = {10.1017/S0305004100023987},
  language = {en},
  number = {01},
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  author = {C. Radhakrishna Rao},
  month = {jan},
  year = {1948},
  pages = {50},
}
@Article{boos1992GeneralizedScoreTests,
  title = {On {{Generalized Score Tests}}},
  volume = {46},
  issn = {00031305},
  doi = {10.2307/2685328},
  language = {en},
  number = {4},
  journal = {The American Statistician},
  author = {Dennis D. Boos},
  month = {nov},
  year = {1992},
  pages = {327},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\EM3BPI2M\\Boos (1992).pdf},
}
@InCollection{tsiatis2006GeometryInfluenceFunctions,
  address = {{New York, NY}},
  series = {Springer {{Series}} in {{Statistics}}},
  title = {The {{Geometry}} of {{Influence Functions}}},
  isbn = {978-0-387-37345-4},
  language = {en},
  booktitle = {Semiparametric {{Theory}} and {{Missing Data}}},
  publisher = {{Springer New York}},
  author = {Anastasios A. Tsiatis},
  year = {2006},
  keywords = {Hilbert Space,Linear Subspace,Maximum Likelihood Estimator,Regularity Condition,Tangent Space},
  pages = {21-51},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\42J55JCI\\Tsiatis - 2006 - The Geometry of Influence Functions.pdf},
  doi = {10.1007/0-387-37345-4_3},
}
@Article{henmi2010ConfidenceIntervalsRandom,
  title = {Confidence Intervals for Random Effects Meta-Analysis and Robustness to Publication Bias},
  volume = {29},
  copyright = {Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  issn = {1097-0258},
  abstract = {The DerSimonian\textendash{}Laird confidence interval for the average treatment effect in meta-analysis is widely used in practice when there is heterogeneity between studies. However, it is well known that its coverage probability (the probability that the interval actually includes the true value) can be substantially below the target level of 95 per cent. It can also be very sensitive to publication bias. In this paper, we propose a new confidence interval that has better coverage than the DerSimonian\textendash{}Laird method, and that is less sensitive to publication bias. The key idea is to note that fixed effects estimates are less sensitive to such biases than random effects estimates, since they put relatively more weight on the larger studies and relatively less weight on the smaller studies. Whereas the DerSimonian\textendash{}Laird interval is centred on a random effects estimate, we centre our confidence interval on a fixed effects estimate, but allow for heterogeneity by including an assessment of the extra uncertainty induced by the random effects setting. Properties of the resulting confidence interval are studied by simulation and compared with other random effects confidence intervals that have been proposed in the literature. An example is briefly discussed. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {29},
  journal = {Statistics in Medicine},
  doi = {10.1002/sim.4029},
  author = {Masayuki Henmi and John B. Copas},
  year = {2010},
  keywords = {DerSimonianâ€“Laird confidence interval,meta-analysis,publication bias,random effects models},
  pages = {2969-2983},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\GDSJ2BB4\\Henmi and Copas - 2010 - Confidence intervals for random effects meta-analy.pdf;C:\\Users\\jep2963\\Zotero\\storage\\9RHK8VFJ\\sim.html},
}
@Article{stanley2015NeitherFixedRandom,
  title = {Neither Fixed nor Random: Weighted Least Squares Meta-Analysis},
  volume = {34},
  issn = {02776715},
  shorttitle = {Neither Fixed nor Random},
  language = {en},
  number = {13},
  journal = {Statistics in Medicine},
  doi = {10.1002/sim.6481},
  author = {T. D. Stanley and Hristos Doucouliagos},
  month = {jun},
  year = {2015},
  pages = {2116-2127},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\6BLEFPN4\\Stanley & Doucouliagos (2015).pdf},
}
@InCollection{rao2005ScoreTestHistorical,
  address = {{Boston, MA}},
  series = {Statistics for {{Industry}} and {{Technology}}},
  title = {Score {{Test}}: {{Historical Review}} and {{Recent Developments}}},
  isbn = {978-0-8176-4422-2},
  shorttitle = {Score {{Test}}},
  abstract = {The three asymptotic tests, Neyman and Pearson Likelihood Ratio (LR), Wald's statistic (W) and Rao's score (RS)are referred to in statistical literature on testing of hypotheses as the Holy Trinity. All these tests are equivalent to the first-order of asymptotics, but differ to some extent in the second-order properties. Some of the merits and defects of these tests are presented.Some applications of the score test, recent developments on refining the score test and problems for further investigation are presented.},
  language = {en},
  booktitle = {Advances in {{Ranking}} and {{Selection}}, {{Multiple Comparisons}}, and {{Reliability}}: {{Methodology}} and {{Applications}}},
  publisher = {{Birkh{\"a}user Boston}},
  author = {C. R. Rao},
  editor = {N. Balakrishnan and H. N. Nagaraja and N. Kannan},
  year = {2005},
  keywords = {Composite hypothesis,Lagrangian multiplier (LM) test,Likelihood ratio (LR),Neyman-Rao test,Neymanâ€™s C(Î±),Raoâ€™s score (RS),Waldâ€™s statistic (W)},
  pages = {3-20},
  file = {C:\\Users\\jep2963\\Zotero\\storage\\XNNYQ2FX\\Rao - 2005 - Score Test Historical Review and Recent Developme.pdf},
  doi = {10.1007/0-8176-4422-9_1},
}
