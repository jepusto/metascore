\documentclass[man,floatsintext]{apa6}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Refining the Excess Significance Test for selective publication in meta-analysis},
            pdfauthor={James E. Pustejovsky},
            pdfkeywords={keywords},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}


  \title{Refining the Excess Significance Test for selective publication in meta-analysis}
    \author{James E. Pustejovsky\textsuperscript{1}}
    \date{}
  
\shorttitle{TES and Vevea-Hedges Selection Models}
\affiliation{
\vspace{0.5cm}
\textsuperscript{1} University of Texas at Austin}
\keywords{keywords\newline\indent Word count: X}
\usepackage{csquotes}
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

\usepackage{longtable}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[flushleft]{threeparttable}
\usepackage{threeparttablex}

\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}
\usepackage{float}
\geometry{twoside=false, top=1in, bottom=1in, left=1in, right=1in}
\usepackage[textwidth=1in, textsize=tiny]{todonotes}
\raggedbottom
\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\bs}{\boldsymbol}

\authornote{James E. Pustejovsky, Educational Psychology Department, University of Texas at Austin.

Correspondence concerning this article should be addressed to James E. Pustejovsky, 1912 Speedway, MS D5800, Austin, TX 78712. E-mail: \href{mailto:pusto@austin.utexas.edu}{\nolinkurl{pusto@austin.utexas.edu}}}

\abstract{
Publication bias and other forms of selective outcome reporting are important threats to the validity of findings from research syntheses---even undermining their special status for informing evidence-based practice and policy guidance. An array of methods have been proposed for detecting selective publication. In particular, Ioannidis and Trikalinos (2007) proposed the Test of Excess Significance (TES), which diagnoses publication bias by comparing the observed number of statistically significant effect sizes to the number expected based on the power of included studies to detect the estimated average effect. Another approach is based on explicit modeling of the selective publication process, as in the weight function model developed by Hedges (1992) and Vevea and Hedges (1995). There is a close connection between these two methods: TES is based on the score function of a simple form of the weight function model. This connection motivates some refinements to TES that improve its operating characteristics and allow for between-study heterogeneity through random effects and regression on study characteristics. After describing the refined tests, I report a small simulation evaluating their calibration and power compared to conventional TES, a likelihood ratio test based on the weight function model, and p-uniform.


}

\begin{document}
\maketitle

Systematic reviews and quantitative research syntheses now lie at the heart of debates about scientific theories and guidance about evidence-based policy.

It has been argued that tests of publication bias are irrelevant and unnecessary because there is overwhelming evidence that selective publication is at work across multiple scientific fields (Morey, 2013).
Need for powerful tests of selective publication.

The remainder of the article proceeds as follows.
In the next section, I briefly review the TES and Vevea-Hedges selection model before developing the connection between the two methods and proposing a refinement to TES.
The following section reports a small Monte Carlo simulation evaluating the size and power of the proposed method.
A brief discussion section highlights limitations and future directions.

\hypertarget{tests}{%
\section{A selection of tests for selective publication}\label{tests}}

Consider a meta-analysis of \(k\) studies, where a conventional random effects model might be applied.
Let \(T_i\) denote the effect size estimate from study \(i\), with standard error \(\sigma_i\), each for \(i = 1,...,k\). Let \(\theta_i\) denote the true effect size parameter from study \(i\).
I shall assume that the included studies are large enough that it is reasonable to treat \(T_i\) as following a normal distribution: \(T_i \sim N(\theta_i, \sigma_i^2)\).
Under a random effects model, and in the absence of selective publication, the true effects are assumed to follow a normal distribution with mean \(\mu\) and standard deviation \(\tau\).

Let \(\alpha\) denote the conventional type-I error level used for one-sided hypothesis tests; in areas of research that typically use two-sided tests and the .05 level to determine significance, the one-sided level would be \(\alpha = .025\).
Let \(\Phi(x)\) and \(\phi(x)\) denote the standard normal cumulative distribution and density function, respectively.
Let \(z_\alpha\) denote the standard normal \(\alpha\)-level critical value, that is, \(z_\alpha = \Phi^{-1}(1 - \alpha)\).
Let \(S_i\) be an indicator for the statistical significance of study \(i\), so that \(S_i = 1\) when \(T_i / \sigma_i > \Phi^{-1}(1 - \alpha)\) and \(S_i = 0\) otherwise.

\hypertarget{test-of-excess-significance}{%
\subsection{Test of Excess Significance}\label{test-of-excess-significance}}

As a test of selective publication, Ioannidis and Trikalinos (2007) proposed to compare the number of statistically significant effects among the set of \(k\) studies to the number of significant effects expected if there were no selection.
The observed number of significant effects is \(O = \sum_{i=1}^K S_i\).
In the absence of selection, the expected number of effects is the sum of the power of each study to detect a true effect of a given size.
Letting \(P_i(\mu,\tau^2)\) denote the power of study \(i\) under a random effects model, which is equal to
\begin{equation} 
P_i(\mu,\tau^2) = 1 - \Phi\left( \frac{\sigma_i z_\alpha - \mu}{\sqrt{\tau^2 + \sigma_i^2}}\right).
\label{eq:power}
\end{equation}
The expected number of significant effects is then \(E(\mu, \tau^2) = \sum_{i=1}^k P_i(\mu, \tau^2)\), a quantity that depends on the unknown average effect \(\mu\) and between-study heterogeneity \(\tau\).
Ioannidis and Trikalinos (2007) suggested estimating expected power based on a fixed effect meta-analysis, taking \(\hat{E} = E(\hat\mu^{FE}, 0)\), where \(\hat\mu^{FE}\) is the usual fixed effect average.
They justify this approach by arguing that heterogeneity is often negligible and that, if there is selective publication, the fixed effect average is less biased than the random effects average.
Subsequent applications of TES have typically followed this approach.\todo{Is this true?}

Ioannidis and Trikalinos (2007) proposed two approximate tests for drawing an inference about whether the set of included studies has been selected for statistical significance. First, they suggest using the test statistic
\begin{equation}
A = \frac{(O - \hat{E})^2}{\hat{E}(k - \hat{E}) / k},
\label{eq:chisq-stat}
\end{equation}
compared to a \(\chi^2_1\) reference distribution.
Alternately, they suggest using a binomial test, comparing \(O\) to a binomial reference distribution with size \(k\) and probability \(\hat{E} / k\).
Applications of TES have typically followed the latter approach.\todo{Is this true?}

Both variants of TES involve approximations. One approximation arises from treating the expected number of studies as known with certainty, whereas in practice it must be estimated. Further, using a binomial reference distribution amounts to assuming that power is constant across studies, which will not be the case unless all included studies are equally precise.
Calculating \(\hat{E}\) under a fixed effect model entails the further assumption that between-study heterogeneity is negligible (Johnson \& Yuan, 2007).
Thus, one might expect that the type I error rate of the tests may be distorted when studies vary in precision or are truly heterogeneous.
Indeed, van Assen, van Aert, and Wicherts (2015) reported simulation results in which TES had below-nominal type I error, even when all studies are equally precise.

TES is an exploratory test for selective publication, intended to be used as a signal that a body of evidence may be unrepresentative (Ioannidis, 2013). It does not, however, invoke any particular model of the selection process. In contrast, other approaches are based on specific models of selective publication.

\hypertarget{weight-function-selection-models}{%
\subsection{Weight function selection models}\label{weight-function-selection-models}}

Many meta-analytic models have been developed that make specific assumptions about the process of selective publication.
One such class of models, often called \enquote{weight function} models, assume that the probability of publication depends on a piece-wise constant function of the statistical significance of the effect size estimate.
Building on earlier work by Iyengar and Greenhouse (1988), Hedges (1992) and Dear and Begg (1992) proposed weight function models that allow for heterogeneity in true effect sizes.
Vevea and Hedges (1995) further developed the approach to allow for moderators of effect size through a meta-regression model.
Based on several extensive Monte Carlo simulations, a very simple, three-parameter version of the weight function model has recently been highlighted as a promising technique for dealing with selective publication (Carter, Schönbrodt, Gervais, \& Hilgard, 2018; McShane, Böckenholt, \& Hansen, 2016).
I limit consideration to this three-parameter model because it is mostly directly connected to TES.
I discuss a more general form of the weight function model in the Appendix.

The weight function model involves two components: a sampling model and a selection model. Following Hedges (1992), the sampling model assumes that effect size estimates follow a basic random effects model as outlined previously. However, not all effect sizes are published (or more generally, not all effect sizes are available for inclusion in the meta-analysis). Rather, the probability that an effect size estimate is included is a multiple of the weight function
\begin{equation}
w(T_i, \sigma_i) = \begin{cases} 1 & \text{if} \quad T_i > \sigma_i z_\alpha \\ \pi & \text{if} \quad T_i \leq \sigma_i z_\alpha \end{cases}
\label{eq:weight-function}
\end{equation}
where \(\pi \geq 0\) is the probability that a statistically insignificant effect size is included, relative to the inclusion probability for an equally precise, statistically significant effect size.
Note that if \(\pi = 1\), then all studies appear with equal probability and there is no selective publication.

The weight function model and tests associated with it are usually based on maximum likelihood estimation models.
Assuming that studies are mutually independent, the joint likelihood of the weight function model is
\begin{equation}
\mathcal{L}(\mu, \tau^2, \pi) = \prod_{i=1}^k \frac{w(T_i, \sigma_i) \phi\left(\frac{T_i - \mu}{\sqrt{\tau^2 + \sigma_i^2}}\right)}{\sqrt{\tau^2 + \sigma_i^2} A_i(\mu, \tau^2, \pi)},
\label{eq:Likelihood}
\end{equation}
where \(A_i\) is a normalizing constant given by
\[
A_i(\mu, \tau^2, \pi) = 1 - (1 - \pi)\Phi\left( \frac{\sigma_i z_\alpha - \mu}{\sqrt{\tau^2 + \sigma_i^2}}\right) = P_i(\mu, \tau^2) + \pi \left[1 - P_i(\mu, \tau^2)\right],
\]
with \(P_i(\mu, \tau^2)\) as given in (\ref{eq:power}). The log likelihood is thus (up to a constant):
\begin{equation}
l(\mu, \tau^2, \pi) = \sum_{i=1}^k \ln w(T_i, \sigma_i) - \frac{1}{2} \sum_{i=1}^k \frac{(T_i - \mu)^2}{\tau^2 + \sigma_i^2} - \frac{1}{2} \ln(\tau^2 + \sigma_i^2) - \sum_{i=1}^k \ln A_i(\mu, \tau^2, \pi).
\label{eq:log-likelihood}
\end{equation}
Let \(\hat\mu\), \(\hat\tau^2\), and \(\hat\pi\) denote the values that maximize \eqref{eq:log-likelihood}---that is, the maximum likelihood estimates of the model parameters.

Hedges (1992) proposed to a test for the null hypothesis that \(\pi = 1\) (i.e., no selective publication) using a likelihood ratio criterion.
Let \(\hat\mu_R\) and \(\hat\tau^2_R\) denote the values that maximize (\ref{eq:log-likelihood}) when \(\pi\) is set equal to 1. The likelihood ratio test statistic is then
\begin{equation}
G^2 = 2 \left[l(\hat\mu, \hat\tau^2, \hat\pi) - l(\hat\mu_R, \hat\tau_R^2, 1)\right],
\label{eq:LRT}
\end{equation}
which is compared to a \(\chi^2_1\) reference distribution.

In practice, a difficulty with the three-parameter weight function model is that maximum likelihood estimates do not converge when all included studies are statistically significant or when no included studies are statistically significant at level \(\alpha\).\todo{Are both of these conditions correct?}
If one of these conditions occurs, a researcher might choose to adjust the \(\alpha\) level defining statistical significance so that at least one study is statistically significant and at least one is statistically insignificant.
I implement this ad hoc modification when evaluating the operating characteristics of the likelihood ratio test in the Monte Carlo simulations.

\hypertarget{refined-excess-significance-tests}{%
\subsection{Refined excess significance tests}\label{refined-excess-significance-tests}}

TES and the three-parameter weight function model are closely connected, in TES is based on the null score of the weight function model.
The score function is the derivative of the log likelihood with respect to its parameters.
Note that the derivative of (\ref{eq:log-likelihood}) with respect to \(\pi\) is
\begin{equation}
S_\pi(\mu, \tau^2, \pi) = \frac{\partial l}{\partial \pi} = \frac{1}{\pi} \sum_{i=1}^k (1 - S_i) - \sum_{i=1}^k \frac{1}{A_i} \Phi\left( \frac{\sigma_i z_\alpha - \mu}{\sqrt{\tau^2 + \sigma_i^2}}\right).
\label{eq:score-pi}
\end{equation}
Under the null hypothesis of \(\pi = 1\), \(A_i = 1\) for \(i = 1,...,k\) and the score simplifies to
\[
\begin{aligned}
S_\pi(\mu, \tau^2, 1) &= \sum_{i=1}^k (1 - S_i) - \sum_{i=1}^k \Phi\left( \frac{\sigma_i z_\alpha - \mu}{\sqrt{\tau^2 + \sigma_i^2}}\right) \\
&= (k - O) - \sum_{i=1}^k \left[1 - P_i(\mu, \tau^2)\right] \\
&= E(\mu,\tau^2) - O.
\end{aligned}
\]
Thus, the score of the three-parameter weight function model, evaluated under the null, is equivalent to the discrepancy between the expected and observed number of significant effects---the same statistic that is used to construct TES.
TES is typically calculated under a fixed effects model, in which case the discrepancy is \(O - \hat{E} = - S_\pi(\hat\mu_{FE}, 0, 1)\). If expected power is instead calculated using the maximum likelihood estimates under a random effects model, then \(O - E(\hat\mu_R, \hat\tau^2_R) = - S_\pi(\hat\mu_R, \hat\tau^2_R, 1)\).

This connection to the weight function model suggests that TES could be refined using score tests, a standard tool from mathematical statistics. Score tests (Rao, 1948) are asymptotically equivalent to likelihood ratio tests, but have the advantage that they do not require obtaining maximum likelihood estimates under the unrestricted model (Boos, 1992). This feature is attractive in the present context because it allows one to circumvent potential convergence problems with the weight function model. Two forms of score tests are available---a parametric form and a robust form---which use different approaches to estimating the variance of the null score.

The parametric score test (Rao, 1948) uses the Fisher information matrix to estimate the variance of the null score. Denote the full parameter vector of the weight function model as \(\boldsymbol\theta = (\mu, \tau^2, \pi)\). Let \(\bm{S}\) be the full score vector, so \(\bm{S}(\boldsymbol\theta) = \partial l(\boldsymbol\theta) / \partial \boldsymbol\theta\), with \(\bm{\tilde{S}}_R = \bm{S}(\hat\mu_R, \hat\tau^2_R, 1)\). Let \(\mathcal{I}\) denote the Fisher information matrix:
\[
\mathcal{I}(\boldsymbol\theta) = - \mathbb{E}\left(\frac{\partial^2 l(\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right),
\]
where \(\mathbb{E}\) denotes the expectation over \(T_1,...,T_k\), and denote \(\tilde{\mathcal{I}}_R = \mathcal{I}(\hat\mu_R, \hat\tau^2_R, 1)\). The parametric score test statistic is then given by
\[
X^2_P = \bm{\tilde{S}}'\tilde{\mathcal{I}}_R \bm{\tilde{S}}.
\]
This quantity can be expressed more directly as:
\begin{equation}
X^2_P = \frac{\left[S_\pi(\hat\mu_R, \hat\tau^2_R, 1)\right]^2}{V_\pi - U_\mu - U_{\tau^2}},
\label{eq:parametric-score}
\end{equation}
where
\[
\begin{aligned}
V_\pi &= \sum_{i=1}^k P_i(\hat\mu_R, \hat\tau^2_R) \left[1 - P_i(\hat\mu_R, \hat\tau^2_R)\right], \\
U_\mu &= \left[\sum_{i=1}^k \frac{1}{\hat\tau_R^2 + \sigma_i^2}\right]^{-1}\left[\sum_{i=1}^k \frac{\phi(c_i)}{\sqrt{\hat\tau^2_R + \sigma^2}}\right]^2, \\
U_{\tau^2} &= \left[\sum_{i=1}^k \frac{1}{\left(\hat\tau_R^2 + \sigma_i^2\right)^2}\right]^{-1}\left[\sum_{i=1}^k \frac{c_i\phi(c_i)}{\hat\tau^2_R + \sigma^2}\right]^2,
\end{aligned}
\]
and \(c_i = \left(\sigma_i z_\alpha - \hat\mu_R\right) / \sqrt{\hat\tau^2_R + \sigma_i^2}\).
The null hypothesis of \(\pi = 1\) is rejected if \(X^2_P\) exceeds a \(\chi^2_1\) critical value.
Alternately, a one-sided, level-\(\alpha_S\) test can be calculated by taking
\begin{equation}
Z_P = \frac{S_\pi(\hat\mu_R, \hat\tau^2_R, 1)}{\sqrt{V_\pi - U_\mu - U_{\tau^2}}}
\end{equation}
and rejecting if \(Z_P < \Phi(\alpha_S)\).

This parametric score test requires using the maximum likelihood estimates (under the restriction of the null hypothesis). Alternative forms of the score test have been described by Kent (1982), White (1982), and Engle (1984), among others (see Boos, 1992 for a review).
These generalized, or robust, forms provide more flexibility in how \(\mu\) and \(\theta\) may be estimated, assuming that there is not selective publication.
Generally, consider estimators \(\dot\mu\) and \(\dot\tau^2\) that are defined as the solutions of estimating equations
\[
\begin{aligned}
S^\mu\left(\mu, \tau^2\right) &= \sum_{i=1}^k S^\mu_i \left(T_i, \sigma_i, \mu, \tau^2\right) = 0, \qquad \text{and} \\
S^{\tau^2}\left(\mu, \tau^2\right) &= \sum_{i=1}^k S^{\tau^2}_i \left(T_i, \sigma_i, \mu, \tau^2\right) = 0.
\end{aligned}
\]
The estimating equation for \(\mu\) will typically take the form
\[
S^\mu(\mu, \tau^2) = \frac{1}{W}\sum_{i=1}^k w_i (T_i - \mu)
\]
for some set of weights \(w_1,...,w_k\) with \(W = \sum_{i=1}^k w_i\).
The fixed effect estimator uses \(w_i = \sigma_i^{-2}\); the random effects estimator uses \(w_i = 1 / \left(\tau^2 + \sigma_i^2\right)\).
For the between-study heterogeneity, many of the available estimators of \(\tau^2\) can be expressed as solutions to estimating equations.
For example, the maximum likelihood estimator uses
\[
S^{\tau^2}(\mu,\tau^2) = \frac{1}{2}\sum_{i=1}^k \frac{(T_i - \mu)^2}{\left(\tau^2 + \sigma_i^2\right)^2} - \frac{1}{2}\sum_{i=1}^k \frac{1}{\tau^2 + \sigma_i^2}.
\]
and the restricted maximum likelihood estimator uses
\[
S^{\tau^2}(\mu,\tau^2) = \frac{1}{2}\sum_{i=1}^k \frac{(T_i - \mu)^2}{\left(\tau^2 + \sigma_i^2\right)^2} + \frac{1}{2 \sum_{i=1}^k (\tau^2 + \sigma_i^2)^{-1}} \sum_{i=1}^k \frac{1}{\left(\tau^2 + \sigma_i^2\right)^2} - \frac{1}{2}\sum_{i=1}^k \frac{1}{\tau^2 + \sigma_i^2}.
\]

\hypertarget{simulations}{%
\section{Size and power comparisons}\label{simulations}}

\begin{itemize}
\tightlist
\item
  TES (FE, chi-sq)
\item
  TES (FE, binom)
\item
  TES (WLS, chi-sq)
\item
  TES (WLS, binom)
\item
  LRT (2-sided)
\item
  LRT (restricting to \(\pi \leq 1\))
\item
  GEST (model-based)
\item
  GEST (robust, ML)
\item
  GEST (robust, WLS)
\end{itemize}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{limitations}{%
\subsection{Limitations}\label{limitations}}

\begin{itemize}
\tightlist
\item
  Independent effects
\item
  Simulations assume 3PSM. Further worked needed on other selection processes, models with precision-effect confounding, etc.
\end{itemize}

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-boos1992GeneralizedScoreTests}{}%
Boos, D. D. (1992). On Generalized Score Tests. \emph{The American Statistician}, \emph{46}(4), 327. doi:\href{https://doi.org/10.2307/2685328}{10.2307/2685328}

\leavevmode\hypertarget{ref-carter2018CorrectingBiasPsychology}{}%
Carter, E. C., Schönbrodt, F. D., Gervais, W. M., \& Hilgard, J. (2018). Correcting for bias in psychology: A comparison of meta-analytic methods. doi:\href{https://doi.org/10.31234/osf.io/9h3nu}{10.31234/osf.io/9h3nu}

\leavevmode\hypertarget{ref-dear1992ApproachAssessingPublication}{}%
Dear, K. B. G., \& Begg, C. B. (1992). An Approach for Assessing Publication Bias Prior to Performing a Meta-Analysis. \emph{Statistical Science}, \emph{7}(2), 237--245.

\leavevmode\hypertarget{ref-hedges1992ModelingPublicationSelection}{}%
Hedges, L. V. (1992). Modeling Publication Selection Effects in Meta-Analysis. \emph{Statistical Science}, \emph{7}(2), 246--255. doi:\href{https://doi.org/10.1214/ss/1177011364}{10.1214/ss/1177011364}

\leavevmode\hypertarget{ref-ioannidis2013ClarificationsApplicationInterpretation}{}%
Ioannidis, J. P. A. (2013). Clarifications on the application and interpretation of the test for excess significance and its extensions. \emph{Journal of Mathematical Psychology}, \emph{57}(5), 184--187. doi:\href{https://doi.org/10.1016/j.jmp.2013.03.002}{10.1016/j.jmp.2013.03.002}

\leavevmode\hypertarget{ref-ioannidis2007ExploratoryTestExcess}{}%
Ioannidis, J. P., \& Trikalinos, T. A. (2007). An exploratory test for an excess of significant findings. \emph{Clinical Trials: Journal of the Society for Clinical Trials}, \emph{4}(3), 245--253. doi:\href{https://doi.org/10.1177/1740774507079441}{10.1177/1740774507079441}

\leavevmode\hypertarget{ref-iyengar1988SelectionModelsFile}{}%
Iyengar, S., \& Greenhouse, J. B. (1988). Selection Models and the File Drawer Problem. \emph{Statistical Science}, \emph{3}(1), 109--117. doi:\href{https://doi.org/10.1214/ss/1177013012}{10.1214/ss/1177013012}

\leavevmode\hypertarget{ref-johnson2007CommentsExploratoryTest}{}%
Johnson, V., \& Yuan, Y. (2007). Comments on ``An exploratory test for an excess of significant findings'' by JPA loannidis and TA Trikalinos. \emph{Clinical Trials: Journal of the Society for Clinical Trials}, \emph{4}(3), 254--255. doi:\href{https://doi.org/10.1177/1740774507079437}{10.1177/1740774507079437}

\leavevmode\hypertarget{ref-mcshane2016AdjustingPublicationBias}{}%
McShane, B. B., Böckenholt, U., \& Hansen, K. T. (2016). Adjusting for Publication Bias in Meta-Analysis: An Evaluation of Selection Methods and Some Cautionary Notes. \emph{Perspectives on Psychological Science}, \emph{11}(5), 730--749. doi:\href{https://doi.org/10.1177/1745691616662243}{10.1177/1745691616662243}

\leavevmode\hypertarget{ref-morey2013ConsistencyTestDoes}{}%
Morey, R. D. (2013). The consistency test does notand cannotDeliver what is advertised: A comment on Francis (2013). \emph{Journal of Mathematical Psychology}, \emph{57}(5), 180--183. doi:\href{https://doi.org/10.1016/j.jmp.2013.03.004}{10.1016/j.jmp.2013.03.004}

\leavevmode\hypertarget{ref-rao1948LargeSampleTests}{}%
Rao, C. R. (1948). Large sample tests of statistical hypotheses concerning several parameters with applications to problems of estimation. \emph{Mathematical Proceedings of the Cambridge Philosophical Society}, \emph{44}(01), 50. doi:\href{https://doi.org/10.1017/S0305004100023987}{10.1017/S0305004100023987}

\leavevmode\hypertarget{ref-vanassen2015MetaanalysisUsingEffect}{}%
van Assen, M. A. L. M., van Aert, R. C. M., \& Wicherts, J. M. (2015). Meta-analysis using effect size distributions of only statistically significant studies. \emph{Psychological Methods}, \emph{20}(3), 293--309. doi:\href{https://doi.org/10.1037/met0000025}{10.1037/met0000025}

\leavevmode\hypertarget{ref-vevea1995GeneralLinearModel}{}%
Vevea, J. L., \& Hedges, L. V. (1995). A general linear model for estimating effect size in the presence of publication bias. \emph{Psychometrika}, \emph{60}(3), 419--435. doi:\href{https://doi.org/10.1007/BF02294384}{10.1007/BF02294384}

\endgroup


\end{document}
